# instalar la utilidad wget para poder descargar de Internet contenido
sudo yum install wget
                 
# con wget descargamos contenido de la bibliotea Gutenberg - Twain and Cooper
wget https://datos.comunidad.madrid/catalogo/dataset/cb5b856f-71a4-4e34-8539-84a7e994c972/resource/9fd86617-370a-4770-8a92-0c42ea02d6a1/download/calidad_aire_datos_dia.csv

                 
# Movemos el fichero
# DS para Deerslayer
# HF para  Huckleberry Finn
mv pg3285.txt DS.txt
mv pg76.txt HF.txt
                 
# Creamos nuestro directorio HdFS
hadoop fs -mkdir -p  /user/DataLake
                 
# AÃ±adimos el fichero al Data Lake
hadoop fs -put HF.txt /user/DataLake/
# Comprobamos si el fichero se encuentra en el directorio
hadoop fs -ls /user/DataLake/                 
                 

hadoop fs -put DS.txt /user/DataLake/

# Se puede trabajar con ficheros comprimidos
gzip DS.txt 
hadoop fs -put DS.txt.gz /user/DataLake/
                 


Contando palabras en Twain y Cooper

                 
# Comprobar la ruta en la que se enuentra hadoop-examples.jar find . -name hadoop-examples.jar
hadoop jar /usr/lib/hadoop-0.20-mapreduce/hadoop-examples.jar wordcount HF.txt HF.out
 
# 
hadoop jar /usr/lib/hadoop/hadoop-examples.jar wordcount HF.txt HF.out
                 
# 
hadoop jar /usr/lib/hadoop-0.20-mapreduce/hadoop-examples.jar wordcount DS.txt.gz DS.out

# Visualizar resultados
hadoop fs -cat /user/cloudera/HF.out/part-r-00000 | less